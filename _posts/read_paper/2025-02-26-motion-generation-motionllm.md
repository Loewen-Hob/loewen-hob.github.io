---
layout: post
title: "motion generation学习进度条（五） motionLLM论文学习笔记"
subtitle: "motionLLM的简单理解"
date: 2025-02-26
author: "hob"
header-img: "img/post-bg-2015.jpg"
tags: ["paper reading","LLM","generative models"]
---
# motionLLM
### 结构
分为两个阶段，第一个阶段可以理解为训练了V-L 翻译器，第二阶段做了指令微调

### 第一阶段：模态翻译 (Modality Translation)
在这一阶段，MotionLLM的主要目标是建立视觉内容与语言之间的桥梁。具体步骤如下：
1. **冻结编码器**：在此阶段，运动编码器和视频编码器，以及语言模型（LLM）都是被冻结的，只有视觉-语言翻译器（V-L translator）是可训练的。
2. **训练翻译器**：该阶段的训练数据包括运动字幕和视频字幕数据。运动翻译器采用线性投影层，而视频翻译器则使用两层多层感知器（MLP），以应对视频数据的复杂性。
3. **目标**：通过训练视觉-语言翻译器，将视觉提示（视频或运动数据）转换为语言嵌入，从而实现模态间的翻译。
这一阶段的训练旨在减少视觉内容与语言之间的模态差距，为后续的指令调整奠定基础。
### 第二阶段：运动-视频统一指令调整 (Motion-Video Unified Instruction Tuning)
在这一阶段，MotionLLM需要处理更为多样化的人类输入指令。具体工作如下：
1. **冻结视觉编码器**：与第一阶段类似，视觉编码器仍然被冻结，而视觉-语言翻译器保持可训练。
2. **指令调整**：这一阶段的训练目标是使模型能够响应不同类型的指令，提升其在实际应用中的灵活性和准确性。
3. **数据处理**：在训练过程中，针对未配对数据集和配对数据集（如MotionX-QA）采用不同的批处理策略，确保每个批次的样本来自同一模态或包含一对运动和视频指令问答。
这一阶段的训练使得模型能够更好地理解和生成与人类运动和视频相关的描述和问答，从而提升其在实际应用中的表现。 
总结来说，第一阶段专注于建立视觉与语言之间的联系，而第二阶段则关注于提升模型对复杂指令的响应能力。